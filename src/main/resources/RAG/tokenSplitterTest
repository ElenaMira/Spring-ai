人工智能正在快速发展。大语言模型（LLM）能够处理自然语言任务，比如对话、翻译、摘要和代码生成。在企业级应用中，常常需要把一篇长文档切分成更小的片段，以便于向量数据库检索和匹配。

例如，在构建 RAG（Retrieval-Augmented Generation）系统时，我们通常会：
1. 将文档读取并转为文本。
2. 使用 TokenTextSplitter 进行分片。
3. 将分片后的文本向量化并存入数据库。
4. 在查询时，检索出最相关的片段，再交给大模型生成回答。

这样就可以避免上下文过长的问题，同时提升检索和问答的准确性。